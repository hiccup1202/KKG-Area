# -*- coding: utf-8 -*-
"""Fine_tune_MaskFormer_on_an_instance_segmentation_dataset_(ADE20k_full)_room_samelabel.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16GIPV1VVNlMIznbrmaYFkJ9M3DyatUfE
"""

# !pip install -q git+https://github.com/huggingface/transformers.git

# !pip install -q datasets albumentations

import json
import os
import random

import cv2  # OpenCVを使用
import numpy as np
import torch
from datasets import Dataset, DatasetDict
from PIL import Image

# 入力ディレクトリと出力ディレクトリの設定
input_dir = "./"  # Labelme形式のJSONと対応する画像が保存されたディレクトリ
output_dir = "./processed_data"  # 出力フォルダ

os.makedirs(f"{output_dir}/images/train", exist_ok=True)
os.makedirs(f"{output_dir}/images/validation", exist_ok=True)
os.makedirs(f"{output_dir}/images/test", exist_ok=True)
os.makedirs(f"{output_dir}/annotations/train", exist_ok=True)
os.makedirs(f"{output_dir}/annotations/validation", exist_ok=True)
os.makedirs(f"{output_dir}/annotations/test", exist_ok=True)

# JSONファイルをリスト化
json_files = [f for f in os.listdir(input_dir) if f.endswith(".json")]

# ファイルをシャッフル
random.shuffle(json_files)

# 分割
train_files = json_files[:48]
validation_files = json_files[48:49]
test_files = json_files[49:]

# ラベル名の収集
def collect_labels(json_files):
    labels = set()
    for json_file in json_files:
        json_path = os.path.join(input_dir, json_file)
        with open(json_path, "r") as f:
            data = json.load(f)
            for shape in data["shapes"]:
                labels.add(shape["label"])
    return sorted(labels)

# ラベル名を収集してid_to_label辞書を作成
unique_labels = collect_labels(json_files)
id_to_label = {idx: label for idx, label in enumerate(unique_labels)}  # 0から始まるクラスID

print("Generated id_to_label mapping:")
print(id_to_label)

# JSONファイルの処理関数
def process_json_file(json_path, split):
    with open(json_path, "r") as f:
        data = json.load(f)

    # 対応する画像ファイルをロード
    image_path = os.path.join(input_dir, data["imagePath"])
    image = Image.open(image_path).convert("RGB")

    # アノテーションマスクを生成
    annotation_mask = np.zeros((image.height, image.width), dtype=np.uint8)
    for shape in data["shapes"]:
        points = shape["points"]
        label = "room"

        # ラベルをクラスIDに変換
        class_id = None
        for k, v in id_to_label.items():
            if v == label:
                class_id = k
                break
        if class_id is None:
            raise ValueError(f"Unknown label: {label}. This should not happen as all labels are collected beforehand.")

        # ポリゴンを塗りつぶす
        points = np.array(points, dtype=np.int32)
        annotation_mask = cv2.fillPoly(annotation_mask, [points], class_id)

    # 保存用のIDを作成
    image_id = os.path.splitext(os.path.basename(json_path))[0]

    # 画像とアノテーションを保存
    image.save(f"{output_dir}/images/{split}/{image_id}.jpg")
    annotation = Image.fromarray(annotation_mask)
    annotation.save(f"{output_dir}/annotations/{split}/{image_id}.png")

# データセットの分割ごとに処理
for split, files in [("train", train_files), ("validation", validation_files), ("test", test_files)]:
    for json_file in files:
        json_path = os.path.join(input_dir, json_file)
        process_json_file(json_path, split)

# DatasetDictを作成
def load_data(split):
    images_dir = os.path.join(output_dir, f"images/{split}")
    annotations_dir = os.path.join(output_dir, f"annotations/{split}")

    images = []
    annotations = []

    for image_file in os.listdir(images_dir):
        image_path = os.path.join(images_dir, image_file)
        annotation_file = image_file.replace(".jpg", ".png")
        annotation_path = os.path.join(annotations_dir, annotation_file)

        # データをロード
        image = Image.open(image_path).convert("RGB")  # 画像をRGB形式に変換
        annotation = Image.open(annotation_path).convert("RGB")

        # データを追加
        images.append(image)
        annotations.append(annotation)

    return {"image": images, "annotation": annotations}

# データセットをロードしてDatasetDictを作成
datasets = {}
for split in ["train", "validation", "test"]:
    data = load_data(split)
    datasets[split] = Dataset.from_dict(data)

dataset_dict = DatasetDict(datasets)

print(dataset_dict)

# mv processed_data/annotations/test/* processed_data/annotations/train/

# mv processed_data/images/test/* processed_data/images/train/

# mv processed_data/annotations/train/2ldk_4.png processed_data/annotations/test/

# mv processed_data/images/train/2ldk_4.jpg processed_data/images/test/

example = datasets['train'][1]
image = example['image']
example['annotation']

instance_seg = np.array(example["annotation"])[:,:,1] # green channel encodes instances
class_id_map = np.array(example["annotation"])[:,:,0] # red channel encodes semantic category
class_labels = np.unique(class_id_map)

# create mapping between instance IDs and semantic category IDs
inst2class = {}
for label in class_labels:
    instance_ids = np.unique(instance_seg[class_id_map == label])
    inst2class.update({i: label for i in instance_ids})
print(inst2class)

from PIL import Image

print("Visualizing instance:", id_to_label[inst2class[7]])

# let's visualize the first instance (ignoring background)
mask = (instance_seg == 7)
visual_mask = (mask * 255).astype(np.uint8)
Image.fromarray(visual_mask)

import numpy as np

seg = np.array(example['annotation'])
# get green channel
instance_seg = seg[:, :, 1]
instance_seg

R = seg[:, :, 0]
G = seg[:, :, 1]
masks = (R / 10).astype(np.int32) * 256 + (G.astype(np.int32))

visual_mask = (masks * 255).astype(np.uint8)
Image.fromarray(visual_mask)

from transformers import Mask2FormerImageProcessor

processor = Mask2FormerImageProcessor(reduce_labels=True, ignore_index=255, do_resize=False, do_rescale=False, do_normalize=False)

import numpy as np
from torch.utils.data import Dataset


class ImageSegmentationDataset(Dataset):
    """Image segmentation dataset."""

    def __init__(self, dataset, processor, transform=None):
        """
        Args:
            dataset
        """
        self.dataset = dataset
        self.processor = processor
        self.transform = transform

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, idx):
        image = np.array(self.dataset[idx]["image"].convert("RGB"))

        instance_seg = np.array(self.dataset[idx]["annotation"])[:,:,1]
        class_id_map = np.array(self.dataset[idx]["annotation"])[:,:,0]
        class_labels = np.unique(class_id_map)

        inst2class = {}
        for label in class_labels:
            instance_ids = np.unique(instance_seg[class_id_map == label])
            inst2class.update({i: label for i in instance_ids})

        # apply transforms
        if self.transform is not None:
            transformed = self.transform(image=image, mask=instance_seg)
            image, instance_seg = transformed['image'], transformed['mask']
            # convert to C, H, W
            image = image.transpose(2,0,1)

        if class_labels.shape[0] == 1 and class_labels[0] == 0:
            # Some image does not have annotation (all ignored)
            inputs = self.processor([image], return_tensors="pt")
            inputs = {k:v.squeeze() for k,v in inputs.items()}
            inputs["class_labels"] = torch.tensor([0])
            inputs["mask_labels"] = torch.zeros((0, inputs["pixel_values"].shape[-2], inputs["pixel_values"].shape[-1]))
        else:
          inputs = self.processor([image], [instance_seg], instance_id_to_semantic_id=inst2class, return_tensors="pt")
          inputs = {k: v.squeeze() if isinstance(v, torch.Tensor) else v[0] for k,v in inputs.items()}

        return inputs

import albumentations as A

# Convert ADE_MEAN and ADE_STD to lists
ADE_MEAN = [0.485, 0.456, 0.406]
ADE_STD = [0.229, 0.224, 0.225]

# note that you can include more fancy data augmentation methods here
train_transform = A.Compose([
    A.Resize(width=512, height=512),
    A.Normalize(mean=ADE_MEAN, std=ADE_STD), # Now uses lists for mean and std
])

train_dataset = ImageSegmentationDataset(datasets["train"], processor=processor, transform=train_transform)

from torch.utils.data import DataLoader


def collate_fn(batch):
    pixel_values = torch.stack([example["pixel_values"] for example in batch])
    pixel_mask = torch.stack([example["pixel_mask"] for example in batch])
    class_labels = [example["class_labels"] for example in batch]
    mask_labels = [example["mask_labels"] for example in batch]
    return {"pixel_values": pixel_values, "pixel_mask": pixel_mask, "class_labels": class_labels, "mask_labels": mask_labels}

train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)

from transformers import Mask2FormerForUniversalSegmentation

# Replace the head of the pre-trained model
# We specify ignore_mismatched_sizes=True to replace the already fine-tuned classification head by a new one
model = Mask2FormerForUniversalSegmentation.from_pretrained("facebook/mask2former-swin-base-coco-instance",
                                                          id2label=id_to_label,
                                                          ignore_mismatched_sizes=True)

import torch
from tqdm.auto import tqdm

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)

running_loss = 0.0
num_samples = 0
for epoch in range(50):
  print("Epoch:", epoch)
  model.train()
  for idx, batch in enumerate(tqdm(train_dataloader)):
      # Reset the parameter gradients
      optimizer.zero_grad()

      # Forward pass
      outputs = model(
              pixel_values=batch["pixel_values"].to(device),
              mask_labels=[labels.to(device) for labels in batch["mask_labels"]],
              class_labels=[labels.to(device) for labels in batch["class_labels"]],
      )

      # Backward propagation
      loss = outputs.loss
      loss.backward()

      batch_size = batch["pixel_values"].size(0)
      running_loss += loss.item()
      num_samples += batch_size

      if idx % 100 == 0:
        print("Loss:", running_loss/num_samples)

      # Optimization
      optimizer.step()

output_model_path = "mask2former_trained"
model.save_pretrained(output_model_path)
processor.save_pretrained(output_model_path)
print(f"Model weights saved to {output_model_path}")

example = datasets['train'][7]
image = example['image']
image

from transformers import MaskFormerImageProcessor

processor = MaskFormerImageProcessor()
inputs = processor(image, return_tensors="pt").to(device)

import torch

# forward pass
with torch.no_grad():
  outputs = model(**inputs)

results = processor.post_process_instance_segmentation(outputs, target_sizes=[image.size[::-1]])[0]

import numpy as np


def get_mask(segmentation, segment_id):
  mask = (segmentation.cpu().numpy() == segment_id)
  visual_mask = (mask * 255).astype(np.uint8)
  visual_mask = Image.fromarray(visual_mask)

  return visual_mask

results['segments_info']

for segment in results['segments_info']:
    print("Visualizing mask for instance:", model.config.id2label[segment['label_id']+1])
    mask = get_mask(results['segmentation'], segment['id'])
    # display(mask)  # Jupyter notebook display function
    print("------")

# セグメントをクラスラベルごとにまとめる関数
def combine_masks_by_class(results):
    """
    同じクラスラベル（label_id）に属するセグメントを1つのマスクとしてまとめる。
    """
    segmentation = results['segmentation'].cpu().numpy()
    segments_info = results['segments_info']

    # クラスごとのマスクを格納する辞書
    class_masks = {}

    for segment in segments_info:
        label_id = segment['label_id']

        # セグメントIDに対応するマスクを取得
        segment_mask = (segmentation == segment['id'])

        # クラスごとにマスクを統合
        if label_id in class_masks:
            class_masks[label_id] |= segment_mask
        else:
            class_masks[label_id] = segment_mask

    # マスクを辞書から画像形式に変換
    for label_id, mask in class_masks.items():
        class_masks[label_id] = Image.fromarray((mask * 255).astype(np.uint8))

    return class_masks

# クラスごとのマスクを生成
class_masks = combine_masks_by_class(results)

# マスクを可視化
for label_id, mask in class_masks.items():
    print(f"Visualizing combined mask for class: {model.config.id2label[label_id + 1]}")
    # display(mask)  # Jupyter notebook display function
    print("------")

# すべてのクラスラベルを1つのマスクに統合する関数
def combine_all_classes(results):
    """
    すべてのセグメントを1つのマスクに統合する。
    """
    segmentation = results['segmentation'].cpu().numpy()
    unified_mask = np.zeros_like(segmentation, dtype=bool)

    # セグメントを統合
    for segment in results['segments_info']:
        segment_mask = (segmentation == segment['id'])
        unified_mask |= segment_mask  # 全体のマスクに統合

    # 統合マスクを画像形式に変換
    unified_mask_image = Image.fromarray((unified_mask * 255).astype(np.uint8))

    return unified_mask_image

# 全体のマスクを生成
unified_mask = combine_all_classes(results)

# 統合された全体のマスクを可視化
print("Visualizing unified mask for all classes:")
# display(unified_mask)  # Jupyter notebook display function

visual_mask = (masks * 255).astype(np.uint8)
Image.fromarray(visual_mask)

example = datasets['test'][0]
image = example['image']
image

from transformers import MaskFormerImageProcessor

processor = MaskFormerImageProcessor()
inputs = processor(image, return_tensors="pt").to(device)

import torch

# forward pass
with torch.no_grad():
  outputs = model(**inputs)

results = processor.post_process_instance_segmentation(outputs, target_sizes=[image.size[::-1]])[0]

# 全体のマスクを生成
unified_mask = combine_all_classes(results)

# 統合された全体のマスクを可視化
print("Visualizing unified mask for all classes:")
# display(unified_mask)  # Jupyter notebook display function

