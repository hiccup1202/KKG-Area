{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JNfCuBRS5oYH"
   },
   "source": [
    "###画像の前処理\n",
    "* KaggleからCubiCasa5kダウンロード\n",
    "* いただいたスクリプトでマスク画像生成\n",
    "* 元画像としてsvg→png変換\n",
    "- 512×512にリサイズ\n",
    "- マスクが複数のフロアがある場合取得できていない（display=Noneをスルーしなくするように変更）\n",
    "- マスクとオリジナル画像のサイズと位置がずれていた（左上を基準にcropすることで一致）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oj2bhRu7NdDH"
   },
   "source": [
    "###訓練中\n",
    "- 格子を全画像に追加\n",
    "- 格子間隔はランダム\n",
    "- 格子を点線化\n",
    "- largeモデルを使用\n",
    "- 斜めに点線を追加\n",
    "- cubicasaのHigh quality architecturalを使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mHBG3ySoYR8x",
    "outputId": "89f2f164-d8e4-4d35-ec70-ea970ffbd2dc"
   },
   "outputs": [],
   "source": [
    "!pip install -q git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IUuY_gI1YVYy",
    "outputId": "8c981013-cbc9-48ca-a662-e7faf86d3d16"
   },
   "outputs": [],
   "source": [
    "!pip install -q datasets albumentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LB6rjBAag3Gj"
   },
   "source": [
    "### 必要に応じて実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oRG931YecpQY"
   },
   "outputs": [],
   "source": [
    "!unzip -q resize_without_noise.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eW2AvFXsZEl9"
   },
   "outputs": [],
   "source": [
    "!unzip -q /content/drive/MyDrive/models/homes_2.zip -d /content/drive/MyDrive/models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NPIwvx70ZtCY"
   },
   "outputs": [],
   "source": [
    "rm -r processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kawkEkTKf3Vi"
   },
   "outputs": [],
   "source": [
    "!unzip -q processed_data_grid.zip -d grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XHdW7WsgrUBf"
   },
   "outputs": [],
   "source": [
    "!unzip -qo /content/drive/MyDrive/models/resize_architectural.zip -d ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yvn1UbiWcFdy"
   },
   "outputs": [],
   "source": [
    "!unzip -qo /content/drive/MyDrive/models/processed_data_high_512_512.zip -d ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jPmNzbmngOLn",
    "outputId": "1289a289-00cf-4879-d848-eb1912541f0c"
   },
   "outputs": [],
   "source": [
    "!zip -rq /content/drive/MyDrive/models/processed_data_high_512_512.zip ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a9mPm81gtVOj"
   },
   "outputs": [],
   "source": [
    "!cp -r processed_data /content/drive/MyDrive/models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YMPP8UYplrbC",
    "outputId": "54480456-77de-425a-a7ff-209c20ef22b0"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nz0xnmges2Ws"
   },
   "outputs": [],
   "source": [
    "!mv processed_data/annotations/validation/* processed_data/annotations/train/\n",
    "!mv processed_data/annotations/test/* processed_data/annotations/train/\n",
    "!mv processed_data/images/validation/* processed_data/images/train/\n",
    "!mv processed_data/images/test/* processed_data/images/train/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-lihw9FdsKoV",
    "outputId": "cdbc8184-3f3a-448e-bd52-7fcdf48115e7"
   },
   "outputs": [],
   "source": [
    "!mv grid/processed_data/images/test/* processed_data/images/test/\n",
    "!mv grid/processed_data/images/validation/* processed_data/images/validation/\n",
    "!mv grid/processed_data/annotations/test/* processed_data/annotations/test/\n",
    "!mv grid/processed_data/annotations/validation/* processed_data/annotations/validation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C2wmglQZt4TW"
   },
   "outputs": [],
   "source": [
    "!rm processed_data/annotations/train/2ldk_5.png\n",
    "!cp processed_data/annotations/train/original_floorplan_1268.png processed_data/annotations/test/\n",
    "!mv processed_data/annotations/train/original_floorplan_1268.png processed_data/annotations/validation/\n",
    "!rm processed_data/images/train/2ldk_5.png\n",
    "!cp processed_data/images/train/original_floorplan_1268.png processed_data/images/test/\n",
    "!mv processed_data/images/train/original_floorplan_1268.png processed_data/images/validation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yzbp9FDRoxKb",
    "outputId": "450bc0ac-22f6-49a8-c38b-5e59568e80c0"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "input_dir = \"./mask2former_resize2/\"  # 元画像があるディレクトリ\n",
    "output_dir = \"./mask2former_resize_resized2\"  # リサイズ後の画像を保存するディレクトリ\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "resize_width = 512\n",
    "resize_height = 512\n",
    "\n",
    "# 指定ディレクトリ内の画像をリサイズして保存\n",
    "for file_name in tqdm(os.listdir(input_dir)):\n",
    "    if file_name.endswith(\".png\") or file_name.endswith(\".jpg\"):\n",
    "        image_path = os.path.join(input_dir, file_name)\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        # リサイズ\n",
    "        image_resized = image.resize((resize_width, resize_height), Image.NEAREST)\n",
    "\n",
    "        # リサイズ後の画像を保存\n",
    "        save_path = os.path.join(output_dir, file_name)\n",
    "        image_resized.save(save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gAVlrX7AhKJF",
    "outputId": "9112ae17-cd6d-41f1-9bee-bc99e2db78bb"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import cv2\n",
    "from datasets import Dataset, DatasetDict\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 入力ディレクトリと出力ディレクトリの設定\n",
    "# ここでは \"original_floorplan.png\" と \"room_mask_color.png\" があるフォルダを指定\n",
    "input_dir = \"./mask2former_resize_resized/\"\n",
    "output_dir = \"./processed_data\"\n",
    "\n",
    "os.makedirs(f\"{output_dir}/images/train\", exist_ok=True)\n",
    "os.makedirs(f\"{output_dir}/images/validation\", exist_ok=True)\n",
    "os.makedirs(f\"{output_dir}/images/test\", exist_ok=True)\n",
    "os.makedirs(f\"{output_dir}/annotations/train\", exist_ok=True)\n",
    "os.makedirs(f\"{output_dir}/annotations/validation\", exist_ok=True)\n",
    "os.makedirs(f\"{output_dir}/annotations/test\", exist_ok=True)\n",
    "\n",
    "# id_to_label辞書: クラスIDを定義 (ここでは 1: \"room\" のみ)\n",
    "id_to_label = {\n",
    "    1: \"room\",\n",
    "}\n",
    "print(\"Generated id_to_label mapping:\", id_to_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7WeMTB9fuNDK",
    "outputId": "5dfa7cda-6af6-4a12-8f63-d1a383f70c59"
   },
   "outputs": [],
   "source": [
    "# For cubicasa5k\n",
    "\n",
    "files = [(f\"original_floorplan_{i}.png\", f\"room_mask_color_{i}.png\") for i in range(1269)]\n",
    "\n",
    "# シャッフル\n",
    "# random.shuffle(files)\n",
    "\n",
    "\n",
    "train_files = files[0:]\n",
    "\n",
    "\n",
    "# カラーインスタンスマスクを読み込んで、annotation_mask に変換する処理\n",
    "def convert_color_mask_to_annotation(color_mask_bgr):\n",
    "    \"\"\"\n",
    "    color_mask_bgr: OpenCVで読み込んだ BGR配列 (height, width, 3)\n",
    "    戻り値: annotation_mask (同じサイズのBGRまたはRGB配列)\n",
    "    \"\"\"\n",
    "    # アウトプット用\n",
    "    height, width, _ = color_mask_bgr.shape\n",
    "    annotation_mask = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "\n",
    "    # BGR -> unique colors\n",
    "    # まずユニークな色を取得\n",
    "    #  shapeを (H*W,3) に潰してからnp.unique() すればOK\n",
    "    unique_colors = np.unique(color_mask_bgr.reshape(-1, 3), axis=0)\n",
    "\n",
    "    instance_id = 0\n",
    "    for c in unique_colors:\n",
    "        # c は (B,G,R) の1ピクセルカラー\n",
    "        # 背景(0,0,0)はスキップ\n",
    "        if np.all(c == [0,0,0]):\n",
    "            continue\n",
    "\n",
    "        # インスタンスIDを1増やす\n",
    "        instance_id += 1\n",
    "\n",
    "        # c と一致するピクセルを抽出\n",
    "        mask = np.all(color_mask_bgr == c, axis=-1)  # shape=(H,W), bool\n",
    "\n",
    "        # アノテーション上では [0, クラスID=1, インスタンスID] という3chに書き込む\n",
    "        annotation_mask[mask] = [0, instance_id, 1]\n",
    "\n",
    "    return annotation_mask\n",
    "\n",
    "# 読み込んで、train/validation/test に分けて保存する関数\n",
    "def process_floorplan_and_mask(floorplan_png, mask_png, split):\n",
    "    \"\"\"\n",
    "    floorplan_png: 元画像(フロア図)のパス\n",
    "    mask_png: 部屋マスク(カラーインスタンス)のパス\n",
    "    split: \"train\"/\"validation\"/\"test\"\n",
    "    \"\"\"\n",
    "\n",
    "    # 画像を読み込み\n",
    "    floorplan_bgr = cv2.imread(floorplan_png, cv2.IMREAD_COLOR)  # BGR\n",
    "    color_mask_bgr = cv2.imread(mask_png, cv2.IMREAD_COLOR)      # BGR\n",
    "\n",
    "    if floorplan_bgr is None:\n",
    "        print(f\"Error: cannot read {floorplan_png}\")\n",
    "        return\n",
    "    if color_mask_bgr is None:\n",
    "        print(f\"Error: cannot read {mask_png}\")\n",
    "        return\n",
    "\n",
    "    # アノテーションマスクを生成\n",
    "    annotation_mask = convert_color_mask_to_annotation(color_mask_bgr)\n",
    "\n",
    "    # 出力ファイル名\n",
    "    base_name = os.path.splitext(os.path.basename(floorplan_png))[0]\n",
    "    image_id = f\"{base_name}\"\n",
    "\n",
    "    # 保存先\n",
    "    image_save_path = f\"{output_dir}/images/{split}/{image_id}.jpg\"\n",
    "    annotation_save_path = f\"{output_dir}/annotations/{split}/{image_id}.png\"\n",
    "\n",
    "    # OpenCVで書き出し\n",
    "    cv2.imwrite(image_save_path, floorplan_bgr)\n",
    "\n",
    "    # アノテーションを保存 (3ch PNG)\n",
    "    cv2.imwrite(annotation_save_path, annotation_mask)\n",
    "\n",
    "\n",
    "# 分割ごとに処理\n",
    "for split, file_list in [(\"train\", train_files),]:\n",
    "    for (floorplan_png, mask_png) in tqdm(file_list):\n",
    "        process_floorplan_and_mask(os.path.join(input_dir, floorplan_png),\n",
    "                                   os.path.join(input_dir, mask_png),\n",
    "                                   split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L8d_pu7myIc2",
    "outputId": "9f33d390-e900-43f1-cdff-f617abb9f8a2"
   },
   "outputs": [],
   "source": [
    "# For cubicasa5k_high\n",
    "input_dir = \"./mask2former_resize_resized2/\"\n",
    "files = [(f\"white_original_floorplan_{i}.png\", f\"room_mask_color_{i}.png\") for i in range(4000)]\n",
    "\n",
    "# シャッフル\n",
    "# random.shuffle(files)\n",
    "\n",
    "\n",
    "train_files = files[0:]\n",
    "\n",
    "\n",
    "# カラーインスタンスマスクを読み込んで、annotation_mask に変換する処理\n",
    "def convert_color_mask_to_annotation(color_mask_bgr):\n",
    "    \"\"\"\n",
    "    color_mask_bgr: OpenCVで読み込んだ BGR配列 (height, width, 3)\n",
    "    戻り値: annotation_mask (同じサイズのBGRまたはRGB配列)\n",
    "    \"\"\"\n",
    "    # アウトプット用\n",
    "    height, width, _ = color_mask_bgr.shape\n",
    "    annotation_mask = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "\n",
    "    # BGR -> unique colors\n",
    "    # まずユニークな色を取得\n",
    "    #  shapeを (H*W,3) に潰してからnp.unique() すればOK\n",
    "    unique_colors = np.unique(color_mask_bgr.reshape(-1, 3), axis=0)\n",
    "\n",
    "    instance_id = 0\n",
    "    for c in unique_colors:\n",
    "        # c は (B,G,R) の1ピクセルカラー\n",
    "        # 背景(0,0,0)はスキップ\n",
    "        if np.all(c == [0,0,0]):\n",
    "            continue\n",
    "\n",
    "        # インスタンスIDを1増やす\n",
    "        instance_id += 1\n",
    "\n",
    "        # c と一致するピクセルを抽出\n",
    "        mask = np.all(color_mask_bgr == c, axis=-1)  # shape=(H,W), bool\n",
    "\n",
    "        # アノテーション上では [0, クラスID=1, インスタンスID] という3chに書き込む\n",
    "        annotation_mask[mask] = [0, instance_id, 1]\n",
    "\n",
    "    return annotation_mask\n",
    "\n",
    "# 読み込んで、train/validation/test に分けて保存する関数\n",
    "def process_floorplan_and_mask(floorplan_png, mask_png, split):\n",
    "    \"\"\"\n",
    "    floorplan_png: 元画像(フロア図)のパス\n",
    "    mask_png: 部屋マスク(カラーインスタンス)のパス\n",
    "    split: \"train\"/\"validation\"/\"test\"\n",
    "    \"\"\"\n",
    "\n",
    "    # 画像を読み込み\n",
    "    floorplan_bgr = cv2.imread(floorplan_png, cv2.IMREAD_COLOR)  # BGR\n",
    "    color_mask_bgr = cv2.imread(mask_png, cv2.IMREAD_COLOR)      # BGR\n",
    "\n",
    "    if floorplan_bgr is None:\n",
    "        print(f\"Error: cannot read {floorplan_png}\")\n",
    "        return\n",
    "    if color_mask_bgr is None:\n",
    "        print(f\"Error: cannot read {mask_png}\")\n",
    "        return\n",
    "\n",
    "    # アノテーションマスクを生成\n",
    "    annotation_mask = convert_color_mask_to_annotation(color_mask_bgr)\n",
    "\n",
    "    # 出力ファイル名\n",
    "    base_name = os.path.splitext(os.path.basename(floorplan_png))[0]\n",
    "    image_id = f\"{base_name}_high\"\n",
    "\n",
    "    # 保存先\n",
    "    image_save_path = f\"{output_dir}/images/{split}/{image_id}.jpg\"\n",
    "    annotation_save_path = f\"{output_dir}/annotations/{split}/{image_id}.png\"\n",
    "\n",
    "    # OpenCVで書き出し\n",
    "    cv2.imwrite(image_save_path, floorplan_bgr)\n",
    "\n",
    "    # アノテーションを保存 (3ch PNG)\n",
    "    cv2.imwrite(annotation_save_path, annotation_mask)\n",
    "\n",
    "\n",
    "# 分割ごとに処理\n",
    "for split, file_list in [(\"train\", train_files),]:\n",
    "    for (floorplan_png, mask_png) in tqdm(file_list):\n",
    "        process_floorplan_and_mask(os.path.join(input_dir, floorplan_png),\n",
    "                                   os.path.join(input_dir, mask_png),\n",
    "                                   split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vhAB5-AXZOVu",
    "outputId": "a19ad070-1d7b-44fe-afad-9e1781749e37"
   },
   "outputs": [],
   "source": [
    "# For homes\n",
    "\n",
    "input_dir = \"/content/drive/MyDrive/models/homes\"  # Labelme形式のJSONと対応する画像が保存されたディレクトリ\n",
    "output_dir = \"./processed_data\"  # 出力フォルダ\n",
    "\n",
    "# JSONファイルをリスト化\n",
    "json_files = [f for f in os.listdir(input_dir) if f.endswith(\".json\")]\n",
    "\n",
    "# ファイルをシャッフル\n",
    "#random.shuffle(json_files)\n",
    "\n",
    "# 分割\n",
    "train_files = json_files[0:]\n",
    "\n",
    "# id_to_label辞書を作成\n",
    "id_to_label = {\n",
    "    1:\"room\",\n",
    "}\n",
    "\n",
    "print(\"Generated id_to_label mapping:\")\n",
    "print(id_to_label)\n",
    "\n",
    "# JSONファイルの処理関数\n",
    "def process_json_file(json_path, split):\n",
    "    with open(json_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # 対応する画像ファイルをロード\n",
    "    image_path = os.path.join(input_dir, data[\"imagePath\"])\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    # アノテーションマスクを生成\n",
    "    annotation_mask = np.zeros((image.height, image.width, 3), dtype=np.uint8)\n",
    "\n",
    "\n",
    "    instance_id = 0\n",
    "    for shape in data[\"shapes\"]:\n",
    "        instance_id += 1\n",
    "        points = shape[\"points\"]\n",
    "\n",
    "        # ポリゴンを塗りつぶす\n",
    "        points = np.array(points, dtype=np.int32)\n",
    "        fill_color = (1, instance_id, 0)\n",
    "        cv2.fillPoly(annotation_mask, [points], fill_color)\n",
    "\n",
    "    # 保存用のIDを作成\n",
    "    image_id = os.path.splitext(os.path.basename(json_path))[0]\n",
    "\n",
    "    # 画像とアノテーションを保存\n",
    "    image.save(f\"{output_dir}/images/{split}/{image_id}.jpg\")\n",
    "    annotation = Image.fromarray(annotation_mask)\n",
    "    annotation.save(f\"{output_dir}/annotations/{split}/{image_id}.png\")\n",
    "\n",
    "# データセットの分割ごとに処理\n",
    "for split, files in [(\"train\", train_files),]:\n",
    "    for json_file in files:\n",
    "        json_path = os.path.join(input_dir, json_file)\n",
    "        process_json_file(json_path, split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "59kKucvGwOVb",
    "outputId": "3dc5d561-bdf5-42f1-e74c-d3baa16218b9"
   },
   "outputs": [],
   "source": [
    "# For valdata\n",
    "\n",
    "input_dir = \"/content/drive/MyDrive/models/valdata\"  # Labelme形式のJSONと対応する画像が保存されたディレクトリ\n",
    "output_dir = \"./processed_data\"  # 出力フォルダ\n",
    "\n",
    "# JSONファイルをリスト化\n",
    "json_files = [f for f in os.listdir(input_dir) if f.endswith(\".json\")]\n",
    "\n",
    "# ファイルをシャッフル\n",
    "#random.shuffle(json_files)\n",
    "\n",
    "# 分割\n",
    "validation_files = json_files[0:]\n",
    "test_files = json_files[0:]\n",
    "\n",
    "# id_to_label辞書を作成\n",
    "id_to_label = {\n",
    "    1:\"room\",\n",
    "}\n",
    "\n",
    "print(\"Generated id_to_label mapping:\")\n",
    "print(id_to_label)\n",
    "\n",
    "# JSONファイルの処理関数\n",
    "def process_json_file(json_path, split):\n",
    "    with open(json_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # 対応する画像ファイルをロード\n",
    "    image_path = os.path.join(input_dir, data[\"imagePath\"])\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    # アノテーションマスクを生成\n",
    "    annotation_mask = np.zeros((image.height, image.width, 3), dtype=np.uint8)\n",
    "\n",
    "\n",
    "    instance_id = 0\n",
    "    for shape in data[\"shapes\"]:\n",
    "        instance_id += 1\n",
    "        points = shape[\"points\"]\n",
    "\n",
    "        # ポリゴンを塗りつぶす\n",
    "        points = np.array(points, dtype=np.int32)\n",
    "        fill_color = (1, instance_id, 0)\n",
    "        cv2.fillPoly(annotation_mask, [points], fill_color)\n",
    "\n",
    "    # 保存用のIDを作成\n",
    "    image_id = os.path.splitext(os.path.basename(json_path))[0]\n",
    "\n",
    "    # 画像とアノテーションを保存\n",
    "    image.save(f\"{output_dir}/images/{split}/{image_id}.jpg\")\n",
    "    annotation = Image.fromarray(annotation_mask)\n",
    "    annotation.save(f\"{output_dir}/annotations/{split}/{image_id}.png\")\n",
    "\n",
    "# データセットの分割ごとに処理\n",
    "for split, files in [(\"validation\", validation_files), (\"test\", test_files)]:\n",
    "    for json_file in files:\n",
    "        json_path = os.path.join(input_dir, json_file)\n",
    "        process_json_file(json_path, split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PKeV8G79hHf_"
   },
   "source": [
    "### ここからは共通の処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9CMRm88fZG_z",
    "outputId": "b7b07d49-b30e-43ba-8b35-5802559e533a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import cv2\n",
    "from datasets import Dataset, DatasetDict\n",
    "from tqdm import tqdm\n",
    "\n",
    "output_dir = \"processed_data\"\n",
    "\n",
    "id_to_label = {\n",
    "    1: \"room\",\n",
    "}\n",
    "\n",
    "# DatasetDictを作成\n",
    "def load_data(split):\n",
    "    images_dir = os.path.join(output_dir, f\"images/{split}\")\n",
    "    annotations_dir = os.path.join(output_dir, f\"annotations/{split}\")\n",
    "\n",
    "    images = []\n",
    "    annotations = []\n",
    "\n",
    "    for image_file in tqdm(os.listdir(images_dir)):\n",
    "        image_path = os.path.join(images_dir, image_file)\n",
    "        annotation_file = image_file.replace(\".jpg\", \".png\")\n",
    "        annotation_path = os.path.join(annotations_dir, annotation_file)\n",
    "\n",
    "        # データをロード\n",
    "        image = Image.open(image_path).convert(\"RGB\")  # 画像をRGB形式に変換\n",
    "        annotation = Image.open(annotation_path).convert(\"RGB\")\n",
    "\n",
    "        # データを追加\n",
    "        images.append(image)\n",
    "        annotations.append(annotation)\n",
    "\n",
    "    # 学習データ(train)の場合のみシャッフルをかける\n",
    "    if split == \"train\":\n",
    "        # (画像, アノテーション) をペアにして1つのリストとしてまとめる\n",
    "        combined = list(zip(images, annotations))\n",
    "        # combined リストをシャッフル\n",
    "        random.shuffle(combined)\n",
    "        # 再度アンパックして images, annotations に戻す\n",
    "        images, annotations = zip(*combined)\n",
    "        # 必要に応じてリスト化しておく\n",
    "        images, annotations = list(images), list(annotations)\n",
    "\n",
    "    return {\"image\": images, \"annotation\": annotations}\n",
    "\n",
    "# データセットをロードしてDatasetDictを作成\n",
    "datasets = {}\n",
    "for split in [\"train\", \"validation\", \"test\"]:\n",
    "    data = load_data(split)\n",
    "    datasets[split] = Dataset.from_dict(data)\n",
    "\n",
    "dataset_dict = DatasetDict(datasets)\n",
    "\n",
    "print(dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CqARSA0-ZQtv"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "example = datasets['train'][1]\n",
    "seg = np.array(example['annotation'])\n",
    "# get green channel\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "\n",
    "instance_seg = seg[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r2IeGR9DZX__",
    "outputId": "0132299b-cc43-4a32-bbdf-eee3cc47a668"
   },
   "outputs": [],
   "source": [
    "np.unique(instance_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b2y0YmfEiZ2F",
    "outputId": "f3d36740-3b48-4c17-f23a-0f53ba8e0569"
   },
   "outputs": [],
   "source": [
    "instance_seg = np.array(example[\"annotation\"])[:,:,1] # green channel encodes instances\n",
    "class_id_map = np.array(example[\"annotation\"])[:,:,0] # red channel encodes semantic category\n",
    "class_labels = np.unique(class_id_map)\n",
    "\n",
    "# create mapping between instance IDs and semantic category IDs\n",
    "inst2class = {}\n",
    "for label in class_labels:\n",
    "    instance_ids = np.unique(instance_seg[class_id_map == label])\n",
    "    inst2class.update({i: label for i in instance_ids})\n",
    "print(inst2class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fwz9wU62maRZ"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from transformers import Mask2FormerImageProcessor\n",
    "\n",
    "processor = Mask2FormerImageProcessor(reduce_labels=True, ignore_index=255, do_resize=False, do_rescale=False, do_normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "quyfuJyOmoti"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ImageSegmentationDataset(Dataset):\n",
    "    \"\"\"Image segmentation dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, dataset, processor, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataset\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.processor = processor\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = np.array(self.dataset[idx][\"image\"].convert(\"RGB\"))\n",
    "\n",
    "        instance_seg = np.array(self.dataset[idx][\"annotation\"])[:,:,1]\n",
    "        class_id_map = np.array(self.dataset[idx][\"annotation\"])[:,:,0]\n",
    "        class_labels = np.unique(class_id_map)\n",
    "\n",
    "        inst2class = {}\n",
    "        for label in class_labels:\n",
    "            instance_ids = np.unique(instance_seg[class_id_map == label])\n",
    "            inst2class.update({i: label for i in instance_ids})\n",
    "\n",
    "        # apply transforms\n",
    "        if self.transform is not None:\n",
    "            transformed = self.transform(image=image, mask=instance_seg)\n",
    "            image, instance_seg = transformed['image'], transformed['mask']\n",
    "            # convert to C, H, W\n",
    "            image = image.transpose(2,0,1)\n",
    "\n",
    "        if class_labels.shape[0] == 1 and class_labels[0] == 0:\n",
    "            # Some image does not have annotation (all ignored)\n",
    "            inputs = self.processor([image], return_tensors=\"pt\")\n",
    "            inputs = {k:v.squeeze() for k,v in inputs.items()}\n",
    "            inputs[\"class_labels\"] = torch.tensor([0])\n",
    "            inputs[\"mask_labels\"] = torch.zeros((0, inputs[\"pixel_values\"].shape[-2], inputs[\"pixel_values\"].shape[-1]))\n",
    "        else:\n",
    "          inputs = self.processor([image], [instance_seg], instance_id_to_semantic_id=inst2class, return_tensors=\"pt\")\n",
    "          inputs = {k: v.squeeze() if isinstance(v, torch.Tensor) else v[0] for k,v in inputs.items()}\n",
    "\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TB7EOLOcWvr0"
   },
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class DiagonalDottedNoise(A.ImageOnlyTransform):\n",
    "    def __init__(\n",
    "        self,\n",
    "        p=1.0,\n",
    "        min_spacing=20,\n",
    "        max_spacing=70,\n",
    "        min_dash_length=1,\n",
    "        max_dash_length=10,\n",
    "        line_thickness=1,\n",
    "        always_apply=False\n",
    "    ):\n",
    "        super().__init__(always_apply=always_apply, p=p)\n",
    "        self.min_spacing = min_spacing\n",
    "        self.max_spacing = max_spacing\n",
    "        self.min_dash_length = min_dash_length\n",
    "        self.max_dash_length = max_dash_length\n",
    "        self.line_thickness = line_thickness\n",
    "\n",
    "    def get_params(self):\n",
    "        spacing = random.randint(self.min_spacing, self.max_spacing)\n",
    "        dash_length = random.randint(self.min_dash_length, self.max_dash_length)\n",
    "        gap_length = random.randint(self.min_dash_length, self.max_dash_length)\n",
    "        return {\n",
    "            \"spacing\": spacing,\n",
    "            \"dash_length\": dash_length,\n",
    "            \"gap_length\": gap_length,\n",
    "        }\n",
    "\n",
    "    def apply(self, img, spacing=50, dash_length=5, gap_length=5, **params):\n",
    "        height, width, _ = img.shape\n",
    "        diagonal_length = int(np.sqrt(height**2 + width**2))\n",
    "\n",
    "        # 左上から右下への斜め線\n",
    "        for offset in range(-diagonal_length, diagonal_length, spacing):\n",
    "            self._draw_dashed_line(img, offset, dash_length, gap_length, direction='left')\n",
    "\n",
    "        # 右上から左下への斜め線\n",
    "        for offset in range(0, diagonal_length * 2, spacing):\n",
    "            self._draw_dashed_line(img, offset, dash_length, gap_length, direction='right', img_width=width)\n",
    "\n",
    "        return img\n",
    "\n",
    "    def _draw_dashed_line(self, img, offset, dash_length, gap_length, direction='left', img_width=None):\n",
    "        color = (0, 0, 0)  # 黒\n",
    "        height, width, _ = img.shape\n",
    "\n",
    "        if direction == 'left':\n",
    "            x0 = max(offset, 0)\n",
    "            y0 = max(-offset, 0)\n",
    "            x1 = min(width, width + offset)\n",
    "            y1 = min(height, height - offset)\n",
    "        elif direction == 'right':\n",
    "            if img_width is None:\n",
    "                raise ValueError(\"img_width must be provided for 'right' direction.\")\n",
    "            x0 = min(offset, img_width)\n",
    "            y0 = max(0, offset - img_width)\n",
    "            x1 = max(0, offset - height)\n",
    "            y1 = min(offset, height)\n",
    "\n",
    "        line_length = int(np.hypot(x1 - x0, y1 - y0))\n",
    "        if line_length == 0:\n",
    "            return  # Skip invalid lines\n",
    "\n",
    "        direction_vector = ((x1 - x0) / line_length, (y1 - y0) / line_length)\n",
    "\n",
    "        current_pos = 0\n",
    "        while current_pos < line_length:\n",
    "            start_point = (\n",
    "                int(x0 + direction_vector[0] * current_pos),\n",
    "                int(y0 + direction_vector[1] * current_pos)\n",
    "            )\n",
    "            current_pos += dash_length\n",
    "            end_point = (\n",
    "                int(x0 + direction_vector[0] * min(current_pos, line_length)),\n",
    "                int(y0 + direction_vector[1] * min(current_pos, line_length))\n",
    "            )\n",
    "            cv2.line(img, start_point, end_point, color, self.line_thickness)\n",
    "            current_pos += gap_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rUCRA6Gvm6B5",
    "outputId": "ccdb3b00-61d6-49bc-d79e-a32c3d882362"
   },
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "import cv2\n",
    "import random\n",
    "\n",
    "class GridNoise(A.ImageOnlyTransform):\n",
    "    \"\"\"\n",
    "    画像にランダムに格子状のノイズを入れるTransform。\n",
    "    - p: このTransformを適用する確率\n",
    "    - min_grid_spacing, max_grid_spacing: 格子の間隔をとる乱数の範囲\n",
    "    - min_dash_length, max_dash_length: dash_length をとる乱数の範囲\n",
    "    - dash_length, gap_length はそれぞれランダムに取りたいので\n",
    "      get_paramsで決定する。\n",
    "    - line_thickness: 線の太さ\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        p=1.0,\n",
    "        min_grid_spacing=20,\n",
    "        max_grid_spacing=70,\n",
    "        min_dash_length=1,\n",
    "        max_dash_length=10,\n",
    "        line_thickness=1,\n",
    "        always_apply=False\n",
    "    ):\n",
    "        super().__init__(always_apply=always_apply, p=p)\n",
    "        self.min_grid_spacing = min_grid_spacing\n",
    "        self.max_grid_spacing = max_grid_spacing\n",
    "        self.min_dash_length = min_dash_length\n",
    "        self.max_dash_length = max_dash_length\n",
    "        self.line_thickness = line_thickness\n",
    "\n",
    "    def get_params(self):\n",
    "        \"\"\"\n",
    "        Albumentations が変換をかけるたびに呼ばれ、\n",
    "        apply() に渡すパラメータが生成される。\n",
    "        \"\"\"\n",
    "        grid_spacing = random.randint(self.min_grid_spacing, self.max_grid_spacing)\n",
    "        dash_length = random.randint(self.min_dash_length, self.max_dash_length)\n",
    "        gap_length = random.randint(self.min_dash_length, self.max_dash_length)\n",
    "        return {\n",
    "            \"grid_spacing\": grid_spacing,\n",
    "            \"dash_length\": dash_length,\n",
    "            \"gap_length\": gap_length,\n",
    "        }\n",
    "\n",
    "    def apply(self, img, grid_spacing=50, dash_length=0, gap_length=0, **params):\n",
    "        \"\"\"\n",
    "        実際に img に点線/格子を描画する。\n",
    "        \"\"\"\n",
    "        height, width, _ = img.shape\n",
    "\n",
    "        # 水平線を描画\n",
    "        for y in range(0, height, grid_spacing):\n",
    "            self._draw_dashed_line_horizontal(img, y, width, dash_length, gap_length)\n",
    "\n",
    "        # 垂直線を描画\n",
    "        for x in range(0, width, grid_spacing):\n",
    "            self._draw_dashed_line_vertical(img, x, height, dash_length, gap_length)\n",
    "\n",
    "        return img\n",
    "\n",
    "    def _draw_dashed_line_horizontal(self, img, y, width, dash_length, gap_length):\n",
    "        color = (0, 0, 0)  # 黒\n",
    "        if dash_length <= 0 or gap_length <= 0:\n",
    "            # dash_length, gap_length が 0 以下なら実線\n",
    "            cv2.line(img, (0, y), (width - 1, y), color, self.line_thickness)\n",
    "        else:\n",
    "            x = 0\n",
    "            while x < width:\n",
    "                x_end = min(x + dash_length, width - 1)\n",
    "                cv2.line(img, (x, y), (x_end, y), color, self.line_thickness)\n",
    "                x = x_end + gap_length\n",
    "\n",
    "    def _draw_dashed_line_vertical(self, img, x, height, dash_length, gap_length):\n",
    "        color = (0, 0, 0)  # 黒\n",
    "        if dash_length <= 0 or gap_length <= 0:\n",
    "            # dash_length, gap_length が 0 以下なら実線\n",
    "            cv2.line(img, (x, 0), (x, height - 1), color, self.line_thickness)\n",
    "        else:\n",
    "            y = 0\n",
    "            while y < height:\n",
    "                y_end = min(y + dash_length, height - 1)\n",
    "                cv2.line(img, (x, y), (x, y_end), color, self.line_thickness)\n",
    "                y = y_end + gap_length\n",
    "\n",
    "# 使い方の例\n",
    "# p=0.5, grid_spacing=50, line_thickness=1\n",
    "# dash_length=10, gap_length=5 なら、\n",
    "# 「線長さ10ピクセル、隙間5ピクセルの破線」を描画する\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(width=512, height=512),\n",
    "    GridNoise(\n",
    "        p=1.0,\n",
    "        min_grid_spacing=20,\n",
    "        max_grid_spacing=70,\n",
    "        min_dash_length=1,\n",
    "        max_dash_length=10,\n",
    "        line_thickness=1\n",
    "    ),\n",
    "    DiagonalDottedNoise(\n",
    "        p=1.0,\n",
    "        min_spacing=70,\n",
    "        max_spacing=250,\n",
    "        min_dash_length=20,\n",
    "        max_dash_length=100,\n",
    "        line_thickness=1\n",
    "    ),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# バリデーション・テストはノイズなし\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(width=512, height=512),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "], is_check_shapes=False)\n",
    "\n",
    "train_dataset = ImageSegmentationDataset(datasets[\"train\"], processor=processor, transform=train_transform)\n",
    "val_dataset = ImageSegmentationDataset(datasets[\"validation\"], processor=processor, transform=val_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l3Qx8sbBm-E-"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_fn(batch):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in batch])\n",
    "    pixel_mask = torch.stack([example[\"pixel_mask\"] for example in batch])\n",
    "    class_labels = [example[\"class_labels\"] for example in batch]\n",
    "    mask_labels = [example[\"mask_labels\"] for example in batch]\n",
    "    return {\"pixel_values\": pixel_values, \"pixel_mask\": pixel_mask, \"class_labels\": class_labels, \"mask_labels\": mask_labels}\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 303,
     "referenced_widgets": [
      "db10d7c78a874034a3620898e3caa516",
      "b82ad19db1c04097a4c458399d44ba09",
      "6cd4ac30ae12423092fd2d83a0b9831b",
      "7de9414c58ab4b7f9451f7aafb1a1fd7",
      "c05def12b7f74fd3995d58f78bacb465",
      "0f0b7172aceb435aaf91907d6877c5ad",
      "76b599a464eb4d9f84b78227eb2c2b0d",
      "8b3716fa1f6149688eae001f1acbefd4",
      "bdbf196156d94e69b6d02875576b15c8",
      "ca528239e8014b60a701a443554a8612",
      "d9957b8901fd438d846f98fa75a6f457",
      "63b8d13c0071417d8cb85ae44befd122",
      "40f736f178544d7dacd2b757543861da",
      "ae8331a7babd424c9b128be843b45310",
      "3eb3d601013e46cb9940c31acb782850",
      "3515d8061fd94d658549fbe4975637c9",
      "f26baa34503844d7a184b2c6c9a97770",
      "402ce5f4d30347b98ba9073d72410eb0",
      "897a82fd0e3c4673a2278879bf8bd888",
      "d5cee1a8a47a4da391651265d2feb2de",
      "4df75ee46b604932a13c8155f8dc1460",
      "8b45eee9597d4bf8a2a69374a0083eed"
     ]
    },
    "id": "vWoShpfKnRuW",
    "outputId": "3f641e84-24ae-417d-d6a2-c81b9409f876"
   },
   "outputs": [],
   "source": [
    "from transformers import Mask2FormerForUniversalSegmentation\n",
    "\n",
    "# Replace the head of the pre-trained model\n",
    "# We specify ignore_mismatched_sizes=True to replace the already fine-tuned classification head by a new one\n",
    "#facebook/mask2former-swin-large-coco-instance\n",
    "#facebook/mask2former-swin-large-ade-panoptic\n",
    "model = Mask2FormerForUniversalSegmentation.from_pretrained(\"facebook/mask2former-swin-large-ade-panoptic\",\n",
    "                                                          id2label=id_to_label,\n",
    "                                                          ignore_mismatched_sizes=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rFhZNJhEfIFO"
   },
   "outputs": [],
   "source": [
    "# すべてのクラスラベルを1つのマスクに統合する関数\n",
    "def combine_all_classes(results):\n",
    "    \"\"\"\n",
    "    すべてのセグメントを1つのマスクに統合する。\n",
    "    \"\"\"\n",
    "    segmentation = results['segmentation'].cpu().numpy()\n",
    "    unified_mask = np.zeros_like(segmentation, dtype=bool)\n",
    "\n",
    "    # セグメントを統合\n",
    "    for segment in results['segments_info']:\n",
    "        segment_mask = (segmentation == segment['id'])\n",
    "        unified_mask |= segment_mask  # 全体のマスクに統合\n",
    "\n",
    "    # 統合マスクを画像形式に変換\n",
    "    unified_mask_image = Image.fromarray((unified_mask * 255).astype(np.uint8))\n",
    "\n",
    "    return unified_mask_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "0DXUO0Ddfnmc",
    "outputId": "a7aa56f0-ec86-44d4-baf1-97f60f0cce25"
   },
   "outputs": [],
   "source": [
    "example = datasets['test'][0]\n",
    "image_test1 = example['image']\n",
    "example = datasets['test'][1]\n",
    "image_test2 = example['image']\n",
    "display(image_test1)\n",
    "display(image_test2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 529
    },
    "id": "f8GT4LuiZMBU",
    "outputId": "186472e1-e340-49d2-e992-cc8cacc9d54f"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1枚だけ画像を表示\n",
    "def show_transformed_image(dataset, processor, transform, device):\n",
    "    idx = random.randint(0, len(dataset) - 1)\n",
    "    original_image = dataset[idx]['image']\n",
    "    annotation = dataset[idx]['annotation']\n",
    "\n",
    "    # transformを適用\n",
    "    transformed = transform(image=np.array(original_image), mask=np.array(annotation)[:,:,1])\n",
    "    transformed_image = transformed['image']\n",
    "\n",
    "    # processorで変換 (Tensorに変換)\n",
    "    inputs = processor(images=transformed_image, return_tensors=\"pt\")\n",
    "\n",
    "    # 表示用に変換\n",
    "    display_image = transformed_image.copy()\n",
    "    display_image = np.clip(display_image * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406]), 0, 1)\n",
    "    display(Image.fromarray((display_image * 255).astype(np.uint8)))\n",
    "\n",
    "show_transformed_image(datasets[\"train\"], processor, train_transform, \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A0fUWDdxnWjB"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import MaskFormerImageProcessor\n",
    "\n",
    "#processor_test = MaskFormerImageProcessor()\n",
    "\n",
    "#model = Mask2FormerForUniversalSegmentation.from_pretrained(\"/content/drive/MyDrive/models/large_200\",\n",
    "#                                                          id2label=id_to_label,\n",
    "#                                                          ignore_mismatched_sizes=True)\n",
    "#model.to(device)\n",
    "#processor = Mask2FormerImageProcessor.from_pretrained(\"/content/drive/MyDrive/models/large_200\",reduce_labels=True, ignore_index=255, do_resize=False, do_rescale=False, do_normalize=False)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
    "\n",
    "checkpoint = torch.load(\"/content/drive/MyDrive/models/large_200/checkpoint.pth\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.to(device)\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "start_epoch = checkpoint['epoch'] + 61\n",
    "best_val_loss = checkpoint['val_loss']\n",
    "\n",
    "\n",
    "running_loss = 0.0\n",
    "num_samples = 0\n",
    "#best_val_loss = float(\"inf\")\n",
    "best_model_path = \"mask2former_best\"\n",
    "num_epochs=200\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  if epoch < start_epoch:\n",
    "    continue\n",
    "  print(\"Epoch:\", epoch)\n",
    "  model.train()\n",
    "\n",
    "  for idx, batch in enumerate(tqdm(train_dataloader)):\n",
    "      # Reset the parameter gradients\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # Forward pass\n",
    "      outputs = model(\n",
    "              pixel_values=batch[\"pixel_values\"].to(device),\n",
    "              mask_labels=[labels.to(device) for labels in batch[\"mask_labels\"]],\n",
    "              class_labels=[labels.to(device) for labels in batch[\"class_labels\"]],\n",
    "      )\n",
    "\n",
    "      # Backward propagation\n",
    "      loss = outputs.loss\n",
    "      loss.backward()\n",
    "\n",
    "      batch_size = batch[\"pixel_values\"].size(0)\n",
    "      running_loss += loss.item()\n",
    "      num_samples += batch_size\n",
    "\n",
    "      if idx % 300 == 0:\n",
    "        print(\"Loss:\", running_loss/num_samples)\n",
    "\n",
    "      # Optimization\n",
    "      optimizer.step()\n",
    "\n",
    "  # ==== Validation ====\n",
    "  model.eval()\n",
    "  val_loss = 0.0\n",
    "  val_samples = 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "      for val_batch in tqdm(val_dataloader, desc=\"Validation\"):\n",
    "          outputs_val = model(\n",
    "              pixel_values=val_batch[\"pixel_values\"].to(device),\n",
    "              mask_labels=[labels.to(device) for labels in val_batch[\"mask_labels\"]],\n",
    "              class_labels=[labels.to(device) for labels in val_batch[\"class_labels\"]],\n",
    "          )\n",
    "\n",
    "          loss_val = outputs_val.loss\n",
    "\n",
    "          batch_size_val = val_batch[\"pixel_values\"].size(0)\n",
    "          val_loss += loss_val.item() * batch_size_val\n",
    "          val_samples += batch_size_val\n",
    "\n",
    "  epoch_val_loss = val_loss / val_samples\n",
    "  print(f\"[Epoch {epoch}] Validation Loss: {epoch_val_loss:.4f}\")\n",
    "\n",
    "  # best_val_lossを更新した場合のみモデルを保存\n",
    "  if epoch_val_loss < best_val_loss:\n",
    "      best_val_loss = epoch_val_loss\n",
    "      print(f\"=> New best model found! Saving model (val_loss: {best_val_loss:.4f})\")\n",
    "      model.save_pretrained(best_model_path)\n",
    "      processor.save_pretrained(best_model_path)\n",
    "\n",
    "  torch.save({\n",
    "      'epoch': epoch,\n",
    "      'model_state_dict': model.state_dict(),\n",
    "      'optimizer_state_dict': optimizer.state_dict(),\n",
    "      'val_loss': best_val_loss,\n",
    "  }, \"/content/drive/MyDrive/models/large_200/checkpoint.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "k02gIeJcDESj"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0f0b7172aceb435aaf91907d6877c5ad": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3515d8061fd94d658549fbe4975637c9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3eb3d601013e46cb9940c31acb782850": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4df75ee46b604932a13c8155f8dc1460",
      "placeholder": "​",
      "style": "IPY_MODEL_8b45eee9597d4bf8a2a69374a0083eed",
      "value": " 866M/866M [00:04&lt;00:00, 195MB/s]"
     }
    },
    "402ce5f4d30347b98ba9073d72410eb0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "40f736f178544d7dacd2b757543861da": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f26baa34503844d7a184b2c6c9a97770",
      "placeholder": "​",
      "style": "IPY_MODEL_402ce5f4d30347b98ba9073d72410eb0",
      "value": "model.safetensors: 100%"
     }
    },
    "4df75ee46b604932a13c8155f8dc1460": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "63b8d13c0071417d8cb85ae44befd122": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_40f736f178544d7dacd2b757543861da",
       "IPY_MODEL_ae8331a7babd424c9b128be843b45310",
       "IPY_MODEL_3eb3d601013e46cb9940c31acb782850"
      ],
      "layout": "IPY_MODEL_3515d8061fd94d658549fbe4975637c9"
     }
    },
    "6cd4ac30ae12423092fd2d83a0b9831b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8b3716fa1f6149688eae001f1acbefd4",
      "max": 82540,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_bdbf196156d94e69b6d02875576b15c8",
      "value": 82540
     }
    },
    "76b599a464eb4d9f84b78227eb2c2b0d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7de9414c58ab4b7f9451f7aafb1a1fd7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ca528239e8014b60a701a443554a8612",
      "placeholder": "​",
      "style": "IPY_MODEL_d9957b8901fd438d846f98fa75a6f457",
      "value": " 82.5k/82.5k [00:00&lt;00:00, 3.66MB/s]"
     }
    },
    "897a82fd0e3c4673a2278879bf8bd888": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8b3716fa1f6149688eae001f1acbefd4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8b45eee9597d4bf8a2a69374a0083eed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ae8331a7babd424c9b128be843b45310": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_897a82fd0e3c4673a2278879bf8bd888",
      "max": 866256864,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d5cee1a8a47a4da391651265d2feb2de",
      "value": 866256864
     }
    },
    "b82ad19db1c04097a4c458399d44ba09": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0f0b7172aceb435aaf91907d6877c5ad",
      "placeholder": "​",
      "style": "IPY_MODEL_76b599a464eb4d9f84b78227eb2c2b0d",
      "value": "config.json: 100%"
     }
    },
    "bdbf196156d94e69b6d02875576b15c8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c05def12b7f74fd3995d58f78bacb465": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ca528239e8014b60a701a443554a8612": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d5cee1a8a47a4da391651265d2feb2de": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d9957b8901fd438d846f98fa75a6f457": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "db10d7c78a874034a3620898e3caa516": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b82ad19db1c04097a4c458399d44ba09",
       "IPY_MODEL_6cd4ac30ae12423092fd2d83a0b9831b",
       "IPY_MODEL_7de9414c58ab4b7f9451f7aafb1a1fd7"
      ],
      "layout": "IPY_MODEL_c05def12b7f74fd3995d58f78bacb465"
     }
    },
    "f26baa34503844d7a184b2c6c9a97770": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
