{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JNfCuBRS5oYH"
   },
   "source": [
    "###画像の前処理\n",
    "* KaggleからCubiCasa5kダウンロード\n",
    "* いただいたスクリプトでマスク画像生成\n",
    "* 元画像としてsvg→png変換\n",
    "- 処理に時間はかかるがリサイズなし\n",
    "- マスクが複数のフロアがある場合取得できていない（display=Noneをスルーしなくするように変更）\n",
    "- マスクとオリジナル画像のサイズと位置がずれていた（左上を基準にcropすることで一致）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oj2bhRu7NdDH"
   },
   "source": [
    "###訓練中\n",
    "- 格子を全画像に追加\n",
    "- 格子を点線化\n",
    "- 格子間隔はランダム"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mHBG3ySoYR8x",
    "outputId": "760eedca-cc64-41a1-ce50-fe4d21ee3b6c"
   },
   "outputs": [],
   "source": [
    "!pip install -q git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IUuY_gI1YVYy",
    "outputId": "becbc9b5-7143-4465-a5f2-b807314c0722"
   },
   "outputs": [],
   "source": [
    "!pip install -q datasets albumentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LB6rjBAag3Gj"
   },
   "source": [
    "### 必要に応じて実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oRG931YecpQY"
   },
   "outputs": [],
   "source": [
    "!unzip -q resize_without_noise.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eW2AvFXsZEl9"
   },
   "outputs": [],
   "source": [
    "!unzip -q homes_2.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NPIwvx70ZtCY"
   },
   "outputs": [],
   "source": [
    "rm -r processed_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kawkEkTKf3Vi"
   },
   "outputs": [],
   "source": [
    "!unzip -q processed_data_grid.zip -d grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XHdW7WsgrUBf"
   },
   "outputs": [],
   "source": [
    "!unzip -qo processed_data3.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jPmNzbmngOLn",
    "outputId": "28280f54-3dd1-4721-e05f-d3e420dd62ee"
   },
   "outputs": [],
   "source": [
    "!zip -r processed_data.zip processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nz0xnmges2Ws"
   },
   "outputs": [],
   "source": [
    "!mv processed_data/annotations/validation/* processed_data/annotations/train/\n",
    "!mv processed_data/annotations/test/* processed_data/annotations/train/\n",
    "!mv processed_data/images/validation/* processed_data/images/train/\n",
    "!mv processed_data/images/test/* processed_data/images/train/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-lihw9FdsKoV",
    "outputId": "cdbc8184-3f3a-448e-bd52-7fcdf48115e7"
   },
   "outputs": [],
   "source": [
    "!mv grid/processed_data/images/test/* processed_data/images/test/\n",
    "!mv grid/processed_data/images/validation/* processed_data/images/validation/\n",
    "!mv grid/processed_data/annotations/test/* processed_data/annotations/test/\n",
    "!mv grid/processed_data/annotations/validation/* processed_data/annotations/validation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C2wmglQZt4TW"
   },
   "outputs": [],
   "source": [
    "!rm processed_data/annotations/train/2ldk_5.png\n",
    "!cp processed_data/annotations/train/original_floorplan_1268.png processed_data/annotations/test/\n",
    "!mv processed_data/annotations/train/original_floorplan_1268.png processed_data/annotations/validation/\n",
    "!rm processed_data/images/train/2ldk_5.png\n",
    "!cp processed_data/images/train/original_floorplan_1268.png processed_data/images/test/\n",
    "!mv processed_data/images/train/original_floorplan_1268.png processed_data/images/validation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gAVlrX7AhKJF",
    "outputId": "a2bae7d9-0287-4ce3-ed3f-8af2d232ea6b"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import cv2\n",
    "from datasets import Dataset, DatasetDict\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 入力ディレクトリと出力ディレクトリの設定\n",
    "# ここでは \"original_floorplan.png\" と \"room_mask_color.png\" があるフォルダを指定\n",
    "input_dir = \"/content/mask2former_resize/\"\n",
    "output_dir = \"./processed_data\"\n",
    "\n",
    "os.makedirs(f\"{output_dir}/images/train\", exist_ok=True)\n",
    "os.makedirs(f\"{output_dir}/images/validation\", exist_ok=True)\n",
    "os.makedirs(f\"{output_dir}/images/test\", exist_ok=True)\n",
    "os.makedirs(f\"{output_dir}/annotations/train\", exist_ok=True)\n",
    "os.makedirs(f\"{output_dir}/annotations/validation\", exist_ok=True)\n",
    "os.makedirs(f\"{output_dir}/annotations/test\", exist_ok=True)\n",
    "\n",
    "# id_to_label辞書: クラスIDを定義 (ここでは 1: \"room\" のみ)\n",
    "id_to_label = {\n",
    "    1: \"room\",\n",
    "}\n",
    "print(\"Generated id_to_label mapping:\", id_to_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "7WeMTB9fuNDK",
    "outputId": "b06ad18a-f407-4e94-d988-33ae409604bd"
   },
   "outputs": [],
   "source": [
    "# For cubicasa5k\n",
    "\n",
    "files = [(f\"original_floorplan_{i}.png\", f\"room_mask_color_{i}.png\") for i in range(1269)]\n",
    "\n",
    "# シャッフル\n",
    "# random.shuffle(files)\n",
    "\n",
    "\n",
    "train_files = files[:-1]\n",
    "validation_files = files[-1:]\n",
    "test_files = files[-1:]\n",
    "\n",
    "\n",
    "# カラーインスタンスマスクを読み込んで、annotation_mask に変換する処理\n",
    "def convert_color_mask_to_annotation(color_mask_bgr):\n",
    "    \"\"\"\n",
    "    color_mask_bgr: OpenCVで読み込んだ BGR配列 (height, width, 3)\n",
    "    戻り値: annotation_mask (同じサイズのBGRまたはRGB配列)\n",
    "    \"\"\"\n",
    "    # アウトプット用\n",
    "    height, width, _ = color_mask_bgr.shape\n",
    "    annotation_mask = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "\n",
    "    # BGR -> unique colors\n",
    "    # まずユニークな色を取得\n",
    "    #  shapeを (H*W,3) に潰してからnp.unique() すればOK\n",
    "    unique_colors = np.unique(color_mask_bgr.reshape(-1, 3), axis=0)\n",
    "\n",
    "    instance_id = 0\n",
    "    for c in unique_colors:\n",
    "        # c は (B,G,R) の1ピクセルカラー\n",
    "        # 背景(0,0,0)はスキップ\n",
    "        if np.all(c == [0,0,0]):\n",
    "            continue\n",
    "\n",
    "        # インスタンスIDを1増やす\n",
    "        instance_id += 1\n",
    "\n",
    "        # c と一致するピクセルを抽出\n",
    "        mask = np.all(color_mask_bgr == c, axis=-1)  # shape=(H,W), bool\n",
    "\n",
    "        # アノテーション上では [0, クラスID=1, インスタンスID] という3chに書き込む\n",
    "        annotation_mask[mask] = [0, instance_id, 1]\n",
    "\n",
    "    return annotation_mask\n",
    "\n",
    "# 読み込んで、train/validation/test に分けて保存する関数\n",
    "def process_floorplan_and_mask(floorplan_png, mask_png, split):\n",
    "    \"\"\"\n",
    "    floorplan_png: 元画像(フロア図)のパス\n",
    "    mask_png: 部屋マスク(カラーインスタンス)のパス\n",
    "    split: \"train\"/\"validation\"/\"test\"\n",
    "    \"\"\"\n",
    "\n",
    "    # 画像を読み込み\n",
    "    floorplan_bgr = cv2.imread(floorplan_png, cv2.IMREAD_COLOR)  # BGR\n",
    "    color_mask_bgr = cv2.imread(mask_png, cv2.IMREAD_COLOR)      # BGR\n",
    "\n",
    "    if floorplan_bgr is None:\n",
    "        print(f\"Error: cannot read {floorplan_png}\")\n",
    "        return\n",
    "    if color_mask_bgr is None:\n",
    "        print(f\"Error: cannot read {mask_png}\")\n",
    "        return\n",
    "\n",
    "    # アノテーションマスクを生成\n",
    "    annotation_mask = convert_color_mask_to_annotation(color_mask_bgr)\n",
    "\n",
    "    # 出力ファイル名\n",
    "    base_name = os.path.splitext(os.path.basename(floorplan_png))[0]\n",
    "    image_id = f\"{base_name}\"\n",
    "\n",
    "    # 保存先\n",
    "    image_save_path = f\"{output_dir}/images/{split}/{image_id}.jpg\"\n",
    "    annotation_save_path = f\"{output_dir}/annotations/{split}/{image_id}.png\"\n",
    "\n",
    "    # OpenCVで書き出し\n",
    "    cv2.imwrite(image_save_path, floorplan_bgr)\n",
    "\n",
    "    # アノテーションを保存 (3ch PNG)\n",
    "    cv2.imwrite(annotation_save_path, annotation_mask)\n",
    "\n",
    "\n",
    "# 分割ごとに処理\n",
    "for split, file_list in [(\"train\", train_files), (\"validation\", validation_files), (\"test\", test_files)]:\n",
    "    for (floorplan_png, mask_png) in tqdm(file_list):\n",
    "        process_floorplan_and_mask(os.path.join(input_dir, floorplan_png),\n",
    "                                   os.path.join(input_dir, mask_png),\n",
    "                                   split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vhAB5-AXZOVu"
   },
   "outputs": [],
   "source": [
    "# For homes\n",
    "\n",
    "input_dir = \"./homes\"  # Labelme形式のJSONと対応する画像が保存されたディレクトリ\n",
    "output_dir = \"./processed_data\"  # 出力フォルダ\n",
    "\n",
    "# JSONファイルをリスト化\n",
    "json_files = [f for f in os.listdir(input_dir) if f.endswith(\".json\")]\n",
    "\n",
    "# ファイルをシャッフル\n",
    "#random.shuffle(json_files)\n",
    "\n",
    "# 分割\n",
    "train_files = json_files[:48]\n",
    "validation_files = json_files[48:49]\n",
    "test_files = json_files[49:]\n",
    "\n",
    "# id_to_label辞書を作成\n",
    "id_to_label = {\n",
    "    1:\"room\",\n",
    "}\n",
    "\n",
    "print(\"Generated id_to_label mapping:\")\n",
    "print(id_to_label)\n",
    "\n",
    "# JSONファイルの処理関数\n",
    "def process_json_file(json_path, split):\n",
    "    with open(json_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # 対応する画像ファイルをロード\n",
    "    image_path = os.path.join(input_dir, data[\"imagePath\"])\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    # アノテーションマスクを生成\n",
    "    annotation_mask = np.zeros((image.height, image.width, 3), dtype=np.uint8)\n",
    "\n",
    "\n",
    "    instance_id = 0\n",
    "    for shape in data[\"shapes\"]:\n",
    "        instance_id += 1\n",
    "        points = shape[\"points\"]\n",
    "\n",
    "        # ポリゴンを塗りつぶす\n",
    "        points = np.array(points, dtype=np.int32)\n",
    "        fill_color = (1, instance_id, 0)\n",
    "        cv2.fillPoly(annotation_mask, [points], fill_color)\n",
    "\n",
    "    # 保存用のIDを作成\n",
    "    image_id = os.path.splitext(os.path.basename(json_path))[0]\n",
    "\n",
    "    # 画像とアノテーションを保存\n",
    "    image.save(f\"{output_dir}/images/{split}/{image_id}.jpg\")\n",
    "    annotation = Image.fromarray(annotation_mask)\n",
    "    annotation.save(f\"{output_dir}/annotations/{split}/{image_id}.png\")\n",
    "\n",
    "# データセットの分割ごとに処理\n",
    "for split, files in [(\"train\", train_files), (\"validation\", validation_files), (\"test\", test_files)]:\n",
    "    for json_file in files:\n",
    "        json_path = os.path.join(input_dir, json_file)\n",
    "        process_json_file(json_path, split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PKeV8G79hHf_"
   },
   "source": [
    "### ここからは共通の処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "id": "9CMRm88fZG_z",
    "outputId": "3c3c34f2-ab56-4566-e9a6-d3951c6963f8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import cv2\n",
    "from datasets import Dataset, DatasetDict\n",
    "from tqdm import tqdm\n",
    "\n",
    "output_dir = \"./processed_data\"\n",
    "\n",
    "id_to_label = {\n",
    "    1: \"room\",\n",
    "}\n",
    "\n",
    "# DatasetDictを作成\n",
    "def load_data(split):\n",
    "    images_dir = os.path.join(output_dir, f\"images/{split}\")\n",
    "    annotations_dir = os.path.join(output_dir, f\"annotations/{split}\")\n",
    "\n",
    "    images = []\n",
    "    annotations = []\n",
    "\n",
    "    for image_file in tqdm(os.listdir(images_dir)):\n",
    "        image_path = os.path.join(images_dir, image_file)\n",
    "        annotation_file = image_file.replace(\".jpg\", \".png\")\n",
    "        annotation_path = os.path.join(annotations_dir, annotation_file)\n",
    "\n",
    "        # データをロード\n",
    "        image = Image.open(image_path).convert(\"RGB\")  # 画像をRGB形式に変換\n",
    "        annotation = Image.open(annotation_path).convert(\"RGB\")\n",
    "\n",
    "        # データを追加\n",
    "        images.append(image)\n",
    "        annotations.append(annotation)\n",
    "\n",
    "    # 学習データ(train)の場合のみシャッフルをかける\n",
    "    if split == \"train\":\n",
    "        # (画像, アノテーション) をペアにして1つのリストとしてまとめる\n",
    "        combined = list(zip(images, annotations))\n",
    "        # combined リストをシャッフル\n",
    "        random.shuffle(combined)\n",
    "        # 再度アンパックして images, annotations に戻す\n",
    "        images, annotations = zip(*combined)\n",
    "        # 必要に応じてリスト化しておく\n",
    "        images, annotations = list(images), list(annotations)\n",
    "\n",
    "    return {\"image\": images, \"annotation\": annotations}\n",
    "\n",
    "# データセットをロードしてDatasetDictを作成\n",
    "datasets = {}\n",
    "for split in [\"train\", \"validation\", \"test\"]:\n",
    "    data = load_data(split)\n",
    "    datasets[split] = Dataset.from_dict(data)\n",
    "\n",
    "dataset_dict = DatasetDict(datasets)\n",
    "\n",
    "print(dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CqARSA0-ZQtv"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "example = datasets['train'][1]\n",
    "seg = np.array(example['annotation'])\n",
    "# get green channel\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "\n",
    "instance_seg = seg[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r2IeGR9DZX__"
   },
   "outputs": [],
   "source": [
    "np.unique(instance_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b2y0YmfEiZ2F"
   },
   "outputs": [],
   "source": [
    "instance_seg = np.array(example[\"annotation\"])[:,:,1] # green channel encodes instances\n",
    "class_id_map = np.array(example[\"annotation\"])[:,:,0] # red channel encodes semantic category\n",
    "class_labels = np.unique(class_id_map)\n",
    "\n",
    "# create mapping between instance IDs and semantic category IDs\n",
    "inst2class = {}\n",
    "for label in class_labels:\n",
    "    instance_ids = np.unique(instance_seg[class_id_map == label])\n",
    "    inst2class.update({i: label for i in instance_ids})\n",
    "print(inst2class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PWFU8OQ8jW7Z"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "\n",
    "# let's visualize the first instance (ignoring background)\n",
    "mask = (instance_seg == 1)\n",
    "visual_mask = (mask * 255).astype(np.uint8)\n",
    "Image.fromarray(visual_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YWnYaeBImNXz"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "seg = np.array(example['annotation'])\n",
    "# get green channel\n",
    "instance_seg = seg[:, :, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jF8GaMKhlLJa"
   },
   "outputs": [],
   "source": [
    "R = seg[:, :, 0]\n",
    "G = seg[:, :, 1]\n",
    "masks = (R / 10).astype(np.int32) * 256 + (G.astype(np.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nqdPmxSbmDj6"
   },
   "outputs": [],
   "source": [
    "visual_mask = (masks * 255).astype(np.uint8)\n",
    "Image.fromarray(visual_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fwz9wU62maRZ"
   },
   "outputs": [],
   "source": [
    "from transformers import Mask2FormerImageProcessor\n",
    "\n",
    "processor = Mask2FormerImageProcessor(reduce_labels=True, ignore_index=255, do_resize=False, do_rescale=False, do_normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "quyfuJyOmoti"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ImageSegmentationDataset(Dataset):\n",
    "    \"\"\"Image segmentation dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, dataset, processor, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataset\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.processor = processor\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = np.array(self.dataset[idx][\"image\"].convert(\"RGB\"))\n",
    "\n",
    "        instance_seg = np.array(self.dataset[idx][\"annotation\"])[:,:,1]\n",
    "        class_id_map = np.array(self.dataset[idx][\"annotation\"])[:,:,0]\n",
    "        class_labels = np.unique(class_id_map)\n",
    "\n",
    "        inst2class = {}\n",
    "        for label in class_labels:\n",
    "            instance_ids = np.unique(instance_seg[class_id_map == label])\n",
    "            inst2class.update({i: label for i in instance_ids})\n",
    "\n",
    "        # apply transforms\n",
    "        if self.transform is not None:\n",
    "            transformed = self.transform(image=image, mask=instance_seg)\n",
    "            image, instance_seg = transformed['image'], transformed['mask']\n",
    "            # convert to C, H, W\n",
    "            image = image.transpose(2,0,1)\n",
    "\n",
    "        if class_labels.shape[0] == 1 and class_labels[0] == 0:\n",
    "            # Some image does not have annotation (all ignored)\n",
    "            inputs = self.processor([image], return_tensors=\"pt\")\n",
    "            inputs = {k:v.squeeze() for k,v in inputs.items()}\n",
    "            inputs[\"class_labels\"] = torch.tensor([0])\n",
    "            inputs[\"mask_labels\"] = torch.zeros((0, inputs[\"pixel_values\"].shape[-2], inputs[\"pixel_values\"].shape[-1]))\n",
    "        else:\n",
    "          inputs = self.processor([image], [instance_seg], instance_id_to_semantic_id=inst2class, return_tensors=\"pt\")\n",
    "          inputs = {k: v.squeeze() if isinstance(v, torch.Tensor) else v[0] for k,v in inputs.items()}\n",
    "\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rUCRA6Gvm6B5"
   },
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "import cv2\n",
    "import random\n",
    "\n",
    "class GridNoise(A.ImageOnlyTransform):\n",
    "    \"\"\"\n",
    "    画像にランダムに格子状のノイズを入れるTransform。\n",
    "    - p: このTransformを適用する確率\n",
    "    - min_grid_spacing, max_grid_spacing: 格子の間隔をとる乱数の範囲\n",
    "    - min_dash_length, max_dash_length: dash_length をとる乱数の範囲\n",
    "    - dash_length, gap_length はそれぞれランダムに取りたいので\n",
    "      get_paramsで決定する。\n",
    "    - line_thickness: 線の太さ\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        p=1.0,\n",
    "        min_grid_spacing=20,\n",
    "        max_grid_spacing=70,\n",
    "        min_dash_length=1,\n",
    "        max_dash_length=10,\n",
    "        line_thickness=1,\n",
    "        always_apply=False\n",
    "    ):\n",
    "        super().__init__(always_apply=always_apply, p=p)\n",
    "        self.min_grid_spacing = min_grid_spacing\n",
    "        self.max_grid_spacing = max_grid_spacing\n",
    "        self.min_dash_length = min_dash_length\n",
    "        self.max_dash_length = max_dash_length\n",
    "        self.line_thickness = line_thickness\n",
    "\n",
    "    def get_params(self):\n",
    "        \"\"\"\n",
    "        Albumentations が変換をかけるたびに呼ばれ、\n",
    "        apply() に渡すパラメータが生成される。\n",
    "        \"\"\"\n",
    "        grid_spacing = random.randint(self.min_grid_spacing, self.max_grid_spacing)\n",
    "        dash_length = random.randint(self.min_dash_length, self.max_dash_length)\n",
    "        gap_length = random.randint(self.min_dash_length, self.max_dash_length)\n",
    "        return {\n",
    "            \"grid_spacing\": grid_spacing,\n",
    "            \"dash_length\": dash_length,\n",
    "            \"gap_length\": gap_length,\n",
    "        }\n",
    "\n",
    "    def apply(self, img, grid_spacing=50, dash_length=0, gap_length=0, **params):\n",
    "        \"\"\"\n",
    "        実際に img に点線/格子を描画する。\n",
    "        \"\"\"\n",
    "        height, width, _ = img.shape\n",
    "\n",
    "        # 水平線を描画\n",
    "        for y in range(0, height, grid_spacing):\n",
    "            self._draw_dashed_line_horizontal(img, y, width, dash_length, gap_length)\n",
    "\n",
    "        # 垂直線を描画\n",
    "        for x in range(0, width, grid_spacing):\n",
    "            self._draw_dashed_line_vertical(img, x, height, dash_length, gap_length)\n",
    "\n",
    "        return img\n",
    "\n",
    "    def _draw_dashed_line_horizontal(self, img, y, width, dash_length, gap_length):\n",
    "        color = (0, 0, 0)  # 黒\n",
    "        if dash_length <= 0 or gap_length <= 0:\n",
    "            # dash_length, gap_length が 0 以下なら実線\n",
    "            cv2.line(img, (0, y), (width - 1, y), color, self.line_thickness)\n",
    "        else:\n",
    "            x = 0\n",
    "            while x < width:\n",
    "                x_end = min(x + dash_length, width - 1)\n",
    "                cv2.line(img, (x, y), (x_end, y), color, self.line_thickness)\n",
    "                x = x_end + gap_length\n",
    "\n",
    "    def _draw_dashed_line_vertical(self, img, x, height, dash_length, gap_length):\n",
    "        color = (0, 0, 0)  # 黒\n",
    "        if dash_length <= 0 or gap_length <= 0:\n",
    "            # dash_length, gap_length が 0 以下なら実線\n",
    "            cv2.line(img, (x, 0), (x, height - 1), color, self.line_thickness)\n",
    "        else:\n",
    "            y = 0\n",
    "            while y < height:\n",
    "                y_end = min(y + dash_length, height - 1)\n",
    "                cv2.line(img, (x, y), (x, y_end), color, self.line_thickness)\n",
    "                y = y_end + gap_length\n",
    "\n",
    "# 使い方の例\n",
    "# p=0.5, grid_spacing=50, line_thickness=1\n",
    "# dash_length=10, gap_length=5 なら、\n",
    "# 「線長さ10ピクセル、隙間5ピクセルの破線」を描画する\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(width=512, height=512),\n",
    "    GridNoise(\n",
    "        p=1.0,\n",
    "        min_grid_spacing=20,\n",
    "        max_grid_spacing=70,\n",
    "        min_dash_length=1,\n",
    "        max_dash_length=10,\n",
    "        line_thickness=1\n",
    "    ),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# バリデーション・テストはノイズなし\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(width=512, height=512),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "], is_check_shapes=False)\n",
    "\n",
    "train_dataset = ImageSegmentationDataset(datasets[\"train\"], processor=processor, transform=train_transform)\n",
    "val_dataset = ImageSegmentationDataset(datasets[\"validation\"], processor=processor, transform=val_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l3Qx8sbBm-E-"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_fn(batch):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in batch])\n",
    "    pixel_mask = torch.stack([example[\"pixel_mask\"] for example in batch])\n",
    "    class_labels = [example[\"class_labels\"] for example in batch]\n",
    "    mask_labels = [example[\"mask_labels\"] for example in batch]\n",
    "    return {\"pixel_values\": pixel_values, \"pixel_mask\": pixel_mask, \"class_labels\": class_labels, \"mask_labels\": mask_labels}\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vWoShpfKnRuW"
   },
   "outputs": [],
   "source": [
    "from transformers import Mask2FormerForUniversalSegmentation\n",
    "\n",
    "# Replace the head of the pre-trained model\n",
    "# We specify ignore_mismatched_sizes=True to replace the already fine-tuned classification head by a new one\n",
    "model = Mask2FormerForUniversalSegmentation.from_pretrained(\"facebook/mask2former-swin-base-coco-instance\",\n",
    "                                                          id2label=id_to_label,\n",
    "                                                          ignore_mismatched_sizes=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rFhZNJhEfIFO"
   },
   "outputs": [],
   "source": [
    "# すべてのクラスラベルを1つのマスクに統合する関数\n",
    "def combine_all_classes(results):\n",
    "    \"\"\"\n",
    "    すべてのセグメントを1つのマスクに統合する。\n",
    "    \"\"\"\n",
    "    segmentation = results['segmentation'].cpu().numpy()\n",
    "    unified_mask = np.zeros_like(segmentation, dtype=bool)\n",
    "\n",
    "    # セグメントを統合\n",
    "    for segment in results['segments_info']:\n",
    "        segment_mask = (segmentation == segment['id'])\n",
    "        unified_mask |= segment_mask  # 全体のマスクに統合\n",
    "\n",
    "    # 統合マスクを画像形式に変換\n",
    "    unified_mask_image = Image.fromarray((unified_mask * 255).astype(np.uint8))\n",
    "\n",
    "    return unified_mask_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0DXUO0Ddfnmc"
   },
   "outputs": [],
   "source": [
    "example = datasets['test'][0]\n",
    "image_test1 = example['image']\n",
    "example = datasets['test'][1]\n",
    "image_test2 = example['image']\n",
    "example = datasets['test'][2]\n",
    "image_test3 = example['image']\n",
    "display(image_test1)\n",
    "display(image_test2)\n",
    "display(image_test3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A0fUWDdxnWjB"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import MaskFormerImageProcessor\n",
    "\n",
    "processor_test = MaskFormerImageProcessor()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
    "\n",
    "running_loss = 0.0\n",
    "num_samples = 0\n",
    "best_val_loss = float(\"inf\")\n",
    "best_model_path = \"mask2former_best\"\n",
    "num_epochs=20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  print(\"Epoch:\", epoch)\n",
    "  model.train()\n",
    "\n",
    "  for idx, batch in enumerate(tqdm(train_dataloader)):\n",
    "      # Reset the parameter gradients\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # Forward pass\n",
    "      outputs = model(\n",
    "              pixel_values=batch[\"pixel_values\"].to(device),\n",
    "              mask_labels=[labels.to(device) for labels in batch[\"mask_labels\"]],\n",
    "              class_labels=[labels.to(device) for labels in batch[\"class_labels\"]],\n",
    "      )\n",
    "\n",
    "      # Backward propagation\n",
    "      loss = outputs.loss\n",
    "      loss.backward()\n",
    "\n",
    "      batch_size = batch[\"pixel_values\"].size(0)\n",
    "      running_loss += loss.item()\n",
    "      num_samples += batch_size\n",
    "\n",
    "      if idx % 300 == 0:\n",
    "        print(\"Loss:\", running_loss/num_samples)\n",
    "\n",
    "      # Optimization\n",
    "      optimizer.step()\n",
    "\n",
    "  # ==== Validation ====\n",
    "  model.eval()\n",
    "  val_loss = 0.0\n",
    "  val_samples = 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "      for val_batch in tqdm(val_dataloader, desc=\"Validation\"):\n",
    "          outputs_val = model(\n",
    "              pixel_values=val_batch[\"pixel_values\"].to(device),\n",
    "              mask_labels=[labels.to(device) for labels in val_batch[\"mask_labels\"]],\n",
    "              class_labels=[labels.to(device) for labels in val_batch[\"class_labels\"]],\n",
    "          )\n",
    "\n",
    "          loss_val = outputs_val.loss\n",
    "\n",
    "          batch_size_val = val_batch[\"pixel_values\"].size(0)\n",
    "          val_loss += loss_val.item() * batch_size_val\n",
    "          val_samples += batch_size_val\n",
    "\n",
    "  epoch_val_loss = val_loss / val_samples\n",
    "  print(f\"[Epoch {epoch}] Validation Loss: {epoch_val_loss:.4f}\")\n",
    "\n",
    "  # best_val_lossを更新した場合のみモデルを保存\n",
    "  if epoch_val_loss < best_val_loss:\n",
    "      best_val_loss = epoch_val_loss\n",
    "      print(f\"=> New best model found! Saving model (val_loss: {best_val_loss:.4f})\")\n",
    "      model.save_pretrained(best_model_path)\n",
    "      processor.save_pretrained(best_model_path)\n",
    "\n",
    "  # 出力してみる\n",
    "  inputs_test1 = processor_test(image_test1, return_tensors=\"pt\").to(device)\n",
    "  inputs_test2 = processor_test(image_test2, return_tensors=\"pt\").to(device)\n",
    "  inputs_test3 = processor_test(image_test3, return_tensors=\"pt\").to(device)\n",
    "  with torch.no_grad():\n",
    "    outputs_test1 = model(**inputs_test1)\n",
    "    outputs_test2 = model(**inputs_test2)\n",
    "    outputs_test3 = model(**inputs_test3)\n",
    "  results_test1 = processor_test.post_process_instance_segmentation(outputs_test1, target_sizes=[image_test1.size[::-1]])[0]\n",
    "  results_test2 = processor_test.post_process_instance_segmentation(outputs_test2, target_sizes=[image_test2.size[::-1]])[0]\n",
    "  results_test3 = processor_test.post_process_instance_segmentation(outputs_test3, target_sizes=[image_test3.size[::-1]])[0]\n",
    "  # 全体のマスクを生成\n",
    "  unified_mask = combine_all_classes(results_test1)\n",
    "  display(unified_mask)\n",
    "  unified_mask = combine_all_classes(results_test2)\n",
    "  display(unified_mask)\n",
    "  unified_mask = combine_all_classes(results_test3)\n",
    "  display(unified_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yWi-KS5pGel_"
   },
   "outputs": [],
   "source": [
    "# 学習時に使ったもの (例)\n",
    "ADE_MEAN = [0.485, 0.456, 0.406]\n",
    "ADE_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "import albumentations as A\n",
    "\n",
    "test_transform_512 = A.Compose([\n",
    "    A.Resize(width=512, height=512),\n",
    "    A.Normalize(mean=ADE_MEAN, std=ADE_STD),\n",
    "], is_check_shapes=False)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def preprocess_like_training(pil_image, transform, device):\n",
    "    \"\"\"\n",
    "    PIL画像に対し:\n",
    "      1) Albumentationsで訓練時と同じリサイズ・正規化を実施\n",
    "      2) (H, W, C) -> (C, H, W) に変換\n",
    "      3) torch.Tensor化 & バッチ次元追加\n",
    "      4) pixel_mask を全1で作成\n",
    "    を行い、モデルに直接渡せるdictを返す。\n",
    "    \"\"\"\n",
    "    # PIL -> NumPy (H, W, C)\n",
    "    image_np = np.array(pil_image, dtype=np.uint8)\n",
    "\n",
    "    # Albumentationsで前処理 (リサイズ + 正規化)\n",
    "    transformed = transform(image=image_np)\n",
    "    image_processed = transformed[\"image\"]     # shape (H, W, C)\n",
    "\n",
    "    # チャンネル次元を先頭に (C, H, W)\n",
    "    image_processed = np.transpose(image_processed, (2, 0, 1))\n",
    "\n",
    "    # torch.Tensor化 & バッチ次元\n",
    "    pixel_values = torch.from_numpy(image_processed).float().unsqueeze(0).to(device)\n",
    "\n",
    "    # pixel_mask: 全部 True (1) のマスクを作成 (サイズは (B=1, H, W))\n",
    "    _, _, h, w = pixel_values.shape\n",
    "    pixel_mask = torch.ones((1, h, w), dtype=torch.bool, device=device)\n",
    "\n",
    "    return {\n",
    "        \"pixel_values\": pixel_values,\n",
    "        \"pixel_mask\": pixel_mask,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1kPM9nfLEKIh"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "\n",
    "# 前処理して再度推論\n",
    "test_image_path = \"test1.png\"\n",
    "image_test4 = Image.open(test_image_path).convert(\"RGB\")\n",
    "test_image_path = \"test2.png\"\n",
    "image_test5 = Image.open(test_image_path).convert(\"RGB\")\n",
    "\n",
    "for test_image in [image_test1, image_test2, image_test3, image_test4, image_test5]:\n",
    "    # Albumentationsによる前処理 (学習時と同じ: 512x512, 正規化)\n",
    "    inputs_test = preprocess_like_training(\n",
    "        pil_image=test_image,\n",
    "        transform=test_transform_512,  # 上で定義した Albumentationsパイプライン\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    # 推論\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs_test = model(**inputs_test)\n",
    "\n",
    "    # post_process_instance_segmentationでマスクを取り出す\n",
    "    # target_sizes=[(高さ, 幅)] は「最終的に出力したいマスクサイズ」を指定\n",
    "    # もし学習時リサイズ(512x512)のまま可視化したいなら (512,512)\n",
    "    # 元の画像サイズに戻したいなら test_image.size[::-1] (H, W)\n",
    "    results_test = processor_test.post_process_instance_segmentation(\n",
    "        outputs_test,\n",
    "        target_sizes=[test_image.size[::-1]]\n",
    "    )[0]\n",
    "\n",
    "    # セグメントのマスクを一つに統合する関数（既存の combine_all_classes など）\n",
    "    unified_mask = combine_all_classes(results_test)\n",
    "\n",
    "    # 結果を可視化\n",
    "    display(unified_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i3zRpISEIoiz"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
